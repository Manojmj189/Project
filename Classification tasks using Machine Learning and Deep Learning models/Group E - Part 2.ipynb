{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72713,"databundleVersionId":7984788,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **GROUP E - PART 2 - EMOTION RECOGNITION CLASSIFICATION TASK**\n# **Overview:**\n\nThe main objective of this assignment is to classify the images of people’s face to detect their emotion (Emotion Recognition). In this assignment, three different models have been performed for Emotion Recognition: Support Vector Machine (SVM) , Deep Neural Network (DNN) and Convolutional Neural Network (CNN). The SVM model achieved an accuracy of around 0.42 while the DNN model achieved an accuracy of around 0.36 and the CNN model obtained an accuracy of about 0.52.\n\nThrough this assignment, the significance of selecting the appropriate models for better performance has been discovered. CNN, which are able to learn spatial data, outperformed the other models in Emotion Recognition. The key takeaway of this assignment is how well CNN handle image data and perform better in identifying the facial expressions. Further, optimizing the architectures and hyperparameters of CNN can potentially attain even better results in Emotion Recognition.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T15:21:50.060291Z","iopub.execute_input":"2024-04-11T15:21:50.060691Z","iopub.status.idle":"2024-04-11T15:21:51.242028Z","shell.execute_reply.started":"2024-04-11T15:21:50.060660Z","shell.execute_reply":"2024-04-11T15:21:51.240818Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cs985-987-emotion-recognition-project-2024/my_emotion_test.csv\n/kaggle/input/cs985-987-emotion-recognition-project-2024/my_emotion_sample_submission.csv\n/kaggle/input/cs985-987-emotion-recognition-project-2024/my_emotion_train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Libraries and Modules used:**\n\n**NumPy -** NumPy is a python library used for performing mathematical computations.\n\n**Pandas -** Pandas is a python library used for the manipulation and analysis of the data.\n\n**keras.utils -** A module that includes a variety of utility functions that are often used in deep learning tasks. 'to_categorical' is a function in the module that is used for one-hot encoding the categorical labels.\n\n**sklearn.model_selection -** model_selection is a funtion in sklearn library that is used for model selection and evaluation, splitting datasets, etc. train_test_split is a utility function in the module that splits data into train and test subsets.\n\n**sklearn.svm -** The scikit-learn (sklearn) library offers a range of tools where the sklearn.svm module is used for implementing Support Vector Machine (SVM) learning algorithms. 'SVC'- Support Vector Classifier is a class within the module that is used for classification tasks.\n\n**sklearn.metrics -** The sklearn.metrics is a module in scikit-learn library that enables the evaluation of machine learning models. 'accuracy_score' and 'classification_report' are the utility functions that determines the results of the evaluation\n\n**matplotlib.pyplot -** Primary plotting library that offers a diverse range of functions and classes to generate various types of plots, including line plots, scatter plots, bar plots, histograms, and more.\n\n**keras.models -** The Keras library consists of various modules that provide classes and functions for the definition of neural network models.\n\n**keras.layers -** This module contained in the Keras library includes a diverse range of layers that are useful in constructing neural network architectures.\n\n**keras.losses -** A module in keras that grants access to a range of loss functions frequently used for the training of neural network models.\n\n**keras.optimizers -** A module in keras that encompasses a range of optimization algorithms utilized for training the neural network models.\n\n**keras.callbacks -** A module that contains a range of callback functions used throughout the training process of neural network models.\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:21:51.244261Z","iopub.execute_input":"2024-04-11T15:21:51.244835Z","iopub.status.idle":"2024-04-11T15:21:51.249899Z","shell.execute_reply.started":"2024-04-11T15:21:51.244796Z","shell.execute_reply":"2024-04-11T15:21:51.248740Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Load and Read the Data:**\n\nThe CSV file named 'my_emotion_train.csv' contains the training data and 'my_emotion_test.csv' contains the test data. The training data and the test data are read from the directory into a pandas dataframe pd which is then assigned to the variables 'tr_data' and 'te_data' respectively.","metadata":{}},{"cell_type":"code","source":"tr_data=pd.read_csv('/kaggle/input/cs985-987-emotion-recognition-project-2024/my_emotion_train.csv')\nte_data=pd.read_csv('/kaggle/input/cs985-987-emotion-recognition-project-2024/my_emotion_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:21:51.251435Z","iopub.execute_input":"2024-04-11T15:21:51.251860Z","iopub.status.idle":"2024-04-11T15:21:58.569723Z","shell.execute_reply.started":"2024-04-11T15:21:51.251823Z","shell.execute_reply":"2024-04-11T15:21:58.568723Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Display the Training and Test data:**\n\nThe first two rows of the dataframe is displayed using head(2) for verifying that the data has been loaded correctly. For example, tr_data.head(2) displays the first two rows in the training data.","metadata":{}},{"cell_type":"code","source":"tr_data.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:21:58.572684Z","iopub.execute_input":"2024-04-11T15:21:58.573126Z","iopub.status.idle":"2024-04-11T15:21:58.591133Z","shell.execute_reply.started":"2024-04-11T15:21:58.573086Z","shell.execute_reply":"2024-04-11T15:21:58.589952Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      id  emotion                                             pixels\n0   9415        6  29 16 18 18 18 20 19 18 17 17 17 18 17 18 17 1...\n1  19109        3  126 154 167 181 188 194 195 194 196 195 198 20...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>emotion</th>\n      <th>pixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9415</td>\n      <td>6</td>\n      <td>29 16 18 18 18 20 19 18 17 17 17 18 17 18 17 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19109</td>\n      <td>3</td>\n      <td>126 154 167 181 188 194 195 194 196 195 198 20...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"te_data.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:21:58.592594Z","iopub.execute_input":"2024-04-11T15:21:58.593037Z","iopub.status.idle":"2024-04-11T15:21:58.603251Z","shell.execute_reply.started":"2024-04-11T15:21:58.592998Z","shell.execute_reply":"2024-04-11T15:21:58.602268Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     id                                             pixels\n0   640  123 109 128 142 103 113 145 179 178 158 177 19...\n1  6081  41 41 41 41 42 42 42 45 39 16 10 10 7 9 9 5 8 ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>pixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>640</td>\n      <td>123 109 128 142 103 113 145 179 178 158 177 19...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6081</td>\n      <td>41 41 41 41 42 42 42 45 39 16 10 10 7 9 9 5 8 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Preprocessing:**\n\nInitializing an empty list 'x_train' to store the preprocessed pixel data. Then, a loop is iterated over each row in 'pixels' column. Within the loop, the string of pixel values is divided into individual values using the space character. The resulting string list is then converted into a NumPy array with the 'float32' datatype. Next, the 1D array of pixel values is reshaped into 3D array with dimensions 48x48x1. The third dimension shows that each image has a grayscale. Lastly, the reshaped pixel data for each image is added to the 'x_train' list.","metadata":{}},{"cell_type":"code","source":"from keras.utils import to_categorical\nx_train=[]\nfor pixels in tr_data[\"pixels\"]:\n    pixels=np.array(pixels.split(' '),dtype='float32')\n    pixels=pixels.reshape(48,48,1)\n    x_train.append(pixels)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:25:30.789780Z","iopub.execute_input":"2024-04-11T18:25:30.790214Z","iopub.status.idle":"2024-04-11T18:25:48.210106Z","shell.execute_reply.started":"2024-04-11T18:25:30.790167Z","shell.execute_reply":"2024-04-11T18:25:48.209083Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"Extracting the target labels (emotion) from the 'tr_data' and assign those labels to 'y_train'. Then, process the input data features. Iterate a loop over each image array 'x' in 'x_train'. Flatten the pixel values as many models expect input data to be flatten.","metadata":{}},{"cell_type":"code","source":"y_train=tr_data[\"emotion\"]\nx_train=np.array([x.flatten() for x in x_train])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:22:29.914550Z","iopub.execute_input":"2024-04-11T15:22:29.915275Z","iopub.status.idle":"2024-04-11T15:22:30.407481Z","shell.execute_reply.started":"2024-04-11T15:22:29.915242Z","shell.execute_reply":"2024-04-11T15:22:30.406202Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Initializing an empty list 'x_test' to store the preprocessed pixel data like the one initialized for the 'x_train'. Then, a loop is iterated over each row in 'pixels' column. Within the loop, the string of pixel values is divided into individual values using the space character. The resulting string list is then converted into a NumPy array with the 'float32' datatype. Next, the 1D array of pixel values is reshaped into 3D array with dimensions 48x48x1. The third dimension shows that each image has a grayscale. Lastly, the reshaped pixel data for each image is added to the 'x_test' list.","metadata":{}},{"cell_type":"code","source":"x_test=[]\nfor pixels in te_data[\"pixels\"]:\n    pixels=np.array(pixels.split(' '),dtype='float32')\n    pixels=pixels.reshape(48,48,1)\n    x_test.append(pixels)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:22:30.409193Z","iopub.execute_input":"2024-04-11T15:22:30.409620Z","iopub.status.idle":"2024-04-11T15:22:34.269250Z","shell.execute_reply.started":"2024-04-11T15:22:30.409580Z","shell.execute_reply":"2024-04-11T15:22:34.268172Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Normalization of Data:**\n\nNormalization serves as a typical preprocessing step in machine learning, particularly in scenarios involving pixel data like images. The primary objective is to standardize the input features to a consistent range, usually spanning from 0 to 1, in order to promote reliable and effective model training.\n\nNormalizing the pixel values of training data stored in 'x_train' and 'x_test' arrays. Each pixel value in the array is divided by 255.0 to scale the values between 0 and 1.","metadata":{}},{"cell_type":"code","source":"x_train=np.array(x_train)/255.0\nx_test=np.array(x_test)/255.0","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:22:34.270478Z","iopub.execute_input":"2024-04-11T15:22:34.270780Z","iopub.status.idle":"2024-04-11T15:22:34.477687Z","shell.execute_reply.started":"2024-04-11T15:22:34.270756Z","shell.execute_reply":"2024-04-11T15:22:34.476452Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Splitting of Data into Training and Validation sets:**\n\nIn Machine learning, it is a standard procedure to divide the dataset into distinct subsets for training and validation. The training subset is utilized to train the model, whereas the validation subset is used to assess the model's performance.\n\nTaking the input features as 'x_train' and target labels as 'y_train'. 'test_size' indicates the allocation of dataset for validation. In this case, it is set to 0.1, signifying that 10% of the data will be utilized for validation. The purpose of setting the 'random_state' parameter to 42 is to ensure reproducibility. 'x_val' and 'y_val' contains the input features and target labels of validation subset respectively.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.1,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:22:34.482361Z","iopub.execute_input":"2024-04-11T15:22:34.483163Z","iopub.status.idle":"2024-04-11T15:22:35.892570Z","shell.execute_reply.started":"2024-04-11T15:22:34.483127Z","shell.execute_reply":"2024-04-11T15:22:35.891459Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# **Models:**\n\n#  1. **Support Vector Machine (SVM):**\n\nThe Support Vector Machine (SVM) model is extensively used as a reliable classifier in emotion recognition. By effectively identifying the most suitable decision boundaries, SVM is able to accurately differentiate between various emotional states using input features.\n\nInitializing the Support Vector Classifier (SVC) with 'kernel' and 'random_state' as parameters. 'rbf' is a popular type of kernel used for mapping input data into high dimensional space. Then, the SVC model is trained with the training data. The 'fit' function modifies the model's parameters in order to minimize the classification error on the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,classification_report\nmodel=SVC(kernel='rbf',random_state=42)\nmodel.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:22:35.894701Z","iopub.execute_input":"2024-04-11T15:22:35.895170Z","iopub.status.idle":"2024-04-11T15:51:43.561678Z","shell.execute_reply.started":"2024-04-11T15:22:35.895130Z","shell.execute_reply":"2024-04-11T15:51:43.560500Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"SVC(random_state=42)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Prediction on Validation set:**\n\nThe trained SVM model is utilized to predict the labels for the validation set (x_val). The SVM model's 'predict' method calculates the predicted labels by taking the input features. The accuracy score is evaluated by comparing the predicted labels (y_pred) with the true labels of the validation set (y_val). The accuracy score is determined by dividing the number of accurately predicted instances by the total number of instances present in the validation set. Finally, print the computed accuracy score to the console.","metadata":{}},{"cell_type":"code","source":"y_pred=model.predict(x_val)\naccuracy=accuracy_score(y_val,y_pred)\nprint(\"Accuracy:\",accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:51:43.563070Z","iopub.execute_input":"2024-04-11T15:51:43.563382Z","iopub.status.idle":"2024-04-11T15:54:47.112920Z","shell.execute_reply.started":"2024-04-11T15:51:43.563356Z","shell.execute_reply":"2024-04-11T15:54:47.111841Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy: 0.42\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Plotting the graph for the accuracy:**\n\nVisualizing the accuracy of the SVM model using bar graph","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.bar([\"Accuracy\"],[accuracy])\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy of SVM Model\")\nplt.ylim(0,1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:33:11.213038Z","iopub.execute_input":"2024-04-11T18:33:11.213486Z","iopub.status.idle":"2024-04-11T18:33:11.384054Z","shell.execute_reply.started":"2024-04-11T18:33:11.213453Z","shell.execute_reply":"2024-04-11T18:33:11.382905Z"},"trusted":true},"execution_count":149,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"Generate and print the comprehensive report that includes various evaluation metrics such as precision, recall, f1-score and support.","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_val,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:54:47.114441Z","iopub.execute_input":"2024-04-11T15:54:47.114802Z","iopub.status.idle":"2024-04-11T15:54:47.138574Z","shell.execute_reply.started":"2024-04-11T15:54:47.114772Z","shell.execute_reply":"2024-04-11T15:54:47.137401Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.41      0.20      0.27       413\n           1       1.00      0.04      0.08        48\n           2       0.36      0.20      0.25       444\n           3       0.47      0.72      0.57       703\n           4       0.30      0.38      0.34       487\n           5       0.57      0.49      0.53       308\n           6       0.40      0.41      0.41       497\n\n    accuracy                           0.42      2900\n   macro avg       0.50      0.35      0.35      2900\nweighted avg       0.43      0.42      0.40      2900\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Making the final predictions on the test data:**\n\nReshaping the test data (x_test) into a two-dimensional array (x_test_flat) that is suitable for making predictions. The function x_test.shape[0] provides the count of samples present in the test data. '-1' indicates the certainty of all elements from the original data. At last, predict the labels for the reshaped test data (x_test_flat) using the trained SVM model.","metadata":{}},{"cell_type":"code","source":"x_test_flat=x_test.reshape(x_test.shape[0],-1)\nfinal_pred=model.predict(x_test_flat)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:54:47.140168Z","iopub.execute_input":"2024-04-11T15:54:47.140597Z","iopub.status.idle":"2024-04-11T16:02:13.024959Z","shell.execute_reply.started":"2024-04-11T15:54:47.140558Z","shell.execute_reply":"2024-04-11T16:02:13.023942Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 2. **Deep Neural Network (DNN):**\n\nThe deep neural network (DNN) model is utilized in emotion recognition to extract complex patterns from input images, allowing for precise classification of emotions through facial expressions.\n\nAssigning the number of classes in the classification problem to the variable 'num_classes'. Instantiate the Sequential class and then assign the resulting instance to the variable 'dnn_model'. This particular model is intended for constructing the deep neural network architecture in layer-by-layer.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten\nnum_classes=7\ndnn_model=Sequential()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:17:13.791367Z","iopub.execute_input":"2024-04-11T17:17:13.791799Z","iopub.status.idle":"2024-04-11T17:17:13.799595Z","shell.execute_reply.started":"2024-04-11T17:17:13.791758Z","shell.execute_reply":"2024-04-11T17:17:13.798361Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Add a flatten layer to the DNN model. The Flatten layer serves the purpose of transforming the input data by flattening it, thereby converting multi-dimensional data into one-dimensional array. This process involves condensing the input tensor into a vector format, enabling it to be seamlessly transferred to the subsequent dense layers. The 'input_shape' parameter defines the shape of the input data that the Flatten layer expects.","metadata":{}},{"cell_type":"code","source":"dnn_model.add(Flatten(input_shape=(48,48,1)))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:17:17.832989Z","iopub.execute_input":"2024-04-11T17:17:17.833376Z","iopub.status.idle":"2024-04-11T17:17:17.845596Z","shell.execute_reply.started":"2024-04-11T17:17:17.833349Z","shell.execute_reply":"2024-04-11T17:17:17.844160Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"Adding a dense layer with 512 neurons to the model. The Rectified Linear Unit (ReLU) activation function is preferred here because of its simplicity and efficiency. 'ReLU' is a highly effective activation function that substitutes negative values with zero while preserving the positive values. This computational operation is simple to execute, thereby enhancing its efficiency in training extensive neural networks. Similarly, introduce two additional dense layers containing 256 and 128 neurons using 'Relu' activation. After the inclusion of each dense layer, a dropout layer is appended with a dropout rate set at 0.2. Dropout is utilized as a regularization strategy to combat overfitting by selectively deactivating a fraction of input units to zero during training.","metadata":{}},{"cell_type":"code","source":"dnn_model.add(Dense(512,activation='relu'))\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(256,activation='relu'))\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128,activation='relu'))\ndnn_model.add(Dropout(0.2))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:17:20.410534Z","iopub.execute_input":"2024-04-11T17:17:20.410976Z","iopub.status.idle":"2024-04-11T17:17:20.484999Z","shell.execute_reply.started":"2024-04-11T17:17:20.410945Z","shell.execute_reply":"2024-04-11T17:17:20.483742Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"Softmax is a function that transforms the initial raw output scores, also known as logits, into probabilities. This conversion is achieved by exponentiating each score and subsequently normalizing them in a way that their sum equals 1. The inclusion of this dense layer with softmax activation function serves as the final layer of the deep neural network (DNN) model.","metadata":{}},{"cell_type":"code","source":"dnn_model.add(Dense(num_classes,activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:17:24.887542Z","iopub.execute_input":"2024-04-11T17:17:24.887969Z","iopub.status.idle":"2024-04-11T17:17:24.911446Z","shell.execute_reply.started":"2024-04-11T17:17:24.887940Z","shell.execute_reply":"2024-04-11T17:17:24.909795Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Creating a new instance of the Stochastic Gradient Descent(SGD) optimizer by specifying certain hyperparameters such as the 'learning_rate' and the 'momentum'. The 'learning rate' is a key factor in deciding the size of the updates applied to the model weights throughout the training process. In this instance, a learning rate of 0.01 has been selected. The 'momentum' parameter plays a significant role in enhancing the acceleration of SGD in the appropriate direction while reducing oscillations. A momentum value of 0.9 has been explicitly defined here. Employing SGD with a momentum setting of 0.9 facilitates faster convergence and more effective navigation through local minima. The compile() function defines the optimizer, loss function, and evaluation metrics that will be utilized during the training. The loss function specified for multi-class classification problems is the categorical cross-entropy loss function. Throughout the training process, the model will assess its performance by employing accuracy as the evaluation metric.","metadata":{}},{"cell_type":"code","source":"from keras.losses import categorical_crossentropy\nfrom keras.optimizers import SGD\nsgd=SGD(learning_rate=0.01,momentum=0.9)\ndnn_model.compile(optimizer=sgd,loss=categorical_crossentropy,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:17:27.586676Z","iopub.execute_input":"2024-04-11T17:17:27.587103Z","iopub.status.idle":"2024-04-11T17:17:27.599009Z","shell.execute_reply.started":"2024-04-11T17:17:27.587071Z","shell.execute_reply":"2024-04-11T17:17:27.597887Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"# * **Reshaping the Input Data:**\n\nThe reshape() method is used to modify the shape of the input array in order to align it with the expected input shape of the model. Reshaping the training input data ('x_train') to have a new shape (-1,48,48,1). The original size of array determines the inference of the first dimension (number of samples), denoted by -1. The dimensions (48,48,1) that follow indicate the height, width, and number of channels of the input images, respectively.\n\n# * **Reshaping the Validation Data:**\n\nSimilar to reshaping the training input data, reshape the validation data ('x_val) to have the shape (-1,48,48,1).\n\n# * **One-Hot Enocding:**\n\nEncode the training labels (y_train) and validation labels (y_val)using one-hot encoding. 1. One-hot encoding is a method used to transform categorical labels into a binary matrix format. In this representation, each category is denoted by a binary vector containing a single '1' at the position corresponding to the category, while the rest of the vector is filled with '0's. 1. 'to_categorical' method transforms the integer-encoded labels into their corresponding one-hot encoded forms.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nx_train_reshaped=x_train.reshape(-1,48,48,1)\nx_val_reshaped=x_val.reshape(-1,48,48,1)\ny_train_encoded=to_categorical(y_train,num_classes=7)\ny_val_encoded=to_categorical(y_val,num_classes=7)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:17:31.898861Z","iopub.execute_input":"2024-04-11T17:17:31.899295Z","iopub.status.idle":"2024-04-11T17:17:31.908084Z","shell.execute_reply.started":"2024-04-11T17:17:31.899265Z","shell.execute_reply":"2024-04-11T17:17:31.906692Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"# **Invoking the fit():**\n\nThe DNN model undergoes training by invoking the fit() method, which utilizes the provided training data (x_train_reshaped, y_train_encoded) and validation data (x_val_reshaped, y_val_encoded). Throughout the training process, the model's weights and biases are adjusted incrementally through the use of backpropagation and optimization technique (SGD). The model's performance on the training data improves gradually as the training process advances through 30 epochs.","metadata":{}},{"cell_type":"code","source":"dnn_model.fit(x_train_reshaped,y_train_encoded,epochs=30,batch_size=32,validation_data=(x_val_reshaped,y_val_encoded))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:25:13.048757Z","iopub.execute_input":"2024-04-11T17:25:13.049205Z","iopub.status.idle":"2024-04-11T17:28:43.518363Z","shell.execute_reply.started":"2024-04-11T17:25:13.049176Z","shell.execute_reply":"2024-04-11T17:28:43.517071Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3875 - loss: 1.5776 - val_accuracy: 0.3690 - val_loss: 1.5959\nEpoch 2/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3712 - loss: 1.5893 - val_accuracy: 0.3638 - val_loss: 1.6056\nEpoch 3/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3808 - loss: 1.5706 - val_accuracy: 0.3621 - val_loss: 1.6103\nEpoch 4/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3849 - loss: 1.5759 - val_accuracy: 0.3634 - val_loss: 1.6189\nEpoch 5/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.3875 - loss: 1.5709 - val_accuracy: 0.3738 - val_loss: 1.5802\nEpoch 6/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3871 - loss: 1.5595 - val_accuracy: 0.3707 - val_loss: 1.5991\nEpoch 7/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.3887 - loss: 1.5653 - val_accuracy: 0.3717 - val_loss: 1.6052\nEpoch 8/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3928 - loss: 1.5479 - val_accuracy: 0.3621 - val_loss: 1.5963\nEpoch 9/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.3977 - loss: 1.5540 - val_accuracy: 0.3603 - val_loss: 1.6428\nEpoch 10/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.3958 - loss: 1.5518 - val_accuracy: 0.3652 - val_loss: 1.5787\nEpoch 11/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4008 - loss: 1.5366 - val_accuracy: 0.3790 - val_loss: 1.5761\nEpoch 12/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4094 - loss: 1.5363 - val_accuracy: 0.3645 - val_loss: 1.6100\nEpoch 13/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4063 - loss: 1.5236 - val_accuracy: 0.3800 - val_loss: 1.5726\nEpoch 14/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4063 - loss: 1.5246 - val_accuracy: 0.3679 - val_loss: 1.6144\nEpoch 15/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.3960 - loss: 1.5300 - val_accuracy: 0.3621 - val_loss: 1.5984\nEpoch 16/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4133 - loss: 1.5177 - val_accuracy: 0.3734 - val_loss: 1.5765\nEpoch 17/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4026 - loss: 1.5222 - val_accuracy: 0.3759 - val_loss: 1.5750\nEpoch 18/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4136 - loss: 1.5139 - val_accuracy: 0.3841 - val_loss: 1.5598\nEpoch 19/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4120 - loss: 1.5072 - val_accuracy: 0.3683 - val_loss: 1.5871\nEpoch 20/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4179 - loss: 1.4966 - val_accuracy: 0.3779 - val_loss: 1.6036\nEpoch 21/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4211 - loss: 1.5018 - val_accuracy: 0.3679 - val_loss: 1.5908\nEpoch 22/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4123 - loss: 1.5072 - val_accuracy: 0.3700 - val_loss: 1.5649\nEpoch 23/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4241 - loss: 1.4938 - val_accuracy: 0.3748 - val_loss: 1.5690\nEpoch 24/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4201 - loss: 1.4851 - val_accuracy: 0.3762 - val_loss: 1.5570\nEpoch 25/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4142 - loss: 1.4930 - val_accuracy: 0.3755 - val_loss: 1.5609\nEpoch 26/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4214 - loss: 1.4846 - val_accuracy: 0.3855 - val_loss: 1.5548\nEpoch 27/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.4253 - loss: 1.4900 - val_accuracy: 0.3703 - val_loss: 1.5749\nEpoch 28/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4223 - loss: 1.4890 - val_accuracy: 0.3807 - val_loss: 1.5590\nEpoch 29/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4348 - loss: 1.4646 - val_accuracy: 0.3814 - val_loss: 1.5455\nEpoch 30/30\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.4350 - loss: 1.4665 - val_accuracy: 0.3617 - val_loss: 1.6013\n","output_type":"stream"},{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7bb198bb7430>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Making final predictions on test data:**\n\nThe DNN model utilizes the predict() method to predict the probabilities of each class for all samples within the test dataset ('x_test'). The argmax() function is subsequently utilized on these predicted probabilities ('dnn_pred') in order to identify the class with the greatest probability for each individual sample, thereby assigning the predicted class label ('pred_labels').","metadata":{}},{"cell_type":"code","source":"dnn_pred=dnn_model.predict(x_test)\npred_labels=np.argmax(dnn_pred,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:28:49.762377Z","iopub.execute_input":"2024-04-11T17:28:49.762789Z","iopub.status.idle":"2024-04-11T17:28:50.748817Z","shell.execute_reply.started":"2024-04-11T17:28:49.762757Z","shell.execute_reply":"2024-04-11T17:28:50.747583Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It is crucial to clear the computational graph and release memory resources linked to previous models and layers when training multiple models in TensorFlow/Keras. The clear_session() method provides a convenient approach to clear the existing Keras session, which includes the computational graph and any state associated with the model.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:47:30.221432Z","iopub.execute_input":"2024-04-11T17:47:30.221821Z","iopub.status.idle":"2024-04-11T17:47:30.272320Z","shell.execute_reply.started":"2024-04-11T17:47:30.221793Z","shell.execute_reply":"2024-04-11T17:47:30.271143Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"Creating an instance of Sequential and assign the resulting instance to the variable 'cnn_model'.","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D,MaxPooling2D\ncnn_model=Sequential()","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:47:37.597068Z","iopub.execute_input":"2024-04-11T17:47:37.597815Z","iopub.status.idle":"2024-04-11T17:47:37.606359Z","shell.execute_reply.started":"2024-04-11T17:47:37.597781Z","shell.execute_reply":"2024-04-11T17:47:37.605020Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# * **Adding Convolutional Layers:**\n\nAdding a 2D convolutional layer to the model that consists of 64 filters with a kernel size of (3,3). Convolutional layers use learnable filters to perform convolutions and extract distinctive features from the input images. The Rectified Linear Unit (ReLU) activation function is used here because of its efficiency. The dimensions used in the input data are 48x48 pixels and they are grayscale consisting of only one channel.\n\n# * **Adding MaxPooling Layers:**\n\nFollowing the Convolutional layer, inserting a 2D MaxPooling layer to the model which reduces the spatial dimensions of the feature maps, helping to extract the most significant features and reduce complexity. Specifying the size of the pooling window, (2,2). Similarly, adding convolutional layers of 128 and 256 filters each followed by a MaxPooling layer.","metadata":{}},{"cell_type":"code","source":"cnn_model.add(Conv2D(64,(3,3),activation='relu',input_shape=(48,48,1)))\ncnn_model.add(MaxPooling2D(pool_size=(2,2)))\ncnn_model.add(Conv2D(128,(3,3),activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2,2)))\ncnn_model.add(Conv2D(256,(3,3),activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2,2)))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:47:40.387482Z","iopub.execute_input":"2024-04-11T17:47:40.387904Z","iopub.status.idle":"2024-04-11T17:47:40.475570Z","shell.execute_reply.started":"2024-04-11T17:47:40.387856Z","shell.execute_reply":"2024-04-11T17:47:40.474315Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"Flatten() in a convolutional neural network (CNN) serves the purpose of transforming the two-dimensional feature maps generated by the convolutional and pooling layers into a single-dimensional vector.","metadata":{}},{"cell_type":"code","source":"cnn_model.add(Flatten())","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:47:44.893798Z","iopub.execute_input":"2024-04-11T17:47:44.894505Z","iopub.status.idle":"2024-04-11T17:47:44.909062Z","shell.execute_reply.started":"2024-04-11T17:47:44.894466Z","shell.execute_reply":"2024-04-11T17:47:44.907832Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"# * **Adding Dense Layers:**\n\nA fully connected layer consisting of 512 neurons is incorporated into the model, with the activation function being Rectified Linear Unit (ReLU). Likewise, adding two more dense layers with 256 and 128 neurons respectively.\n\n# * **Adding Dropout Layers:**\n\nAfter each dense layer, add a dropout layer with a dropout rate of 0.2 which can be used to avoid overfitting during the training process where a portion of input units is randomly set to zero.","metadata":{}},{"cell_type":"code","source":"cnn_model.add(Dense(512,activation='relu'))\ncnn_model.add(Dropout(0.2))\ncnn_model.add(Dense(256,activation='relu'))\ncnn_model.add(Dropout(0.2))\ncnn_model.add(Dense(128,activation='relu'))\ncnn_model.add(Dropout(0.2))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:47:47.597253Z","iopub.execute_input":"2024-04-11T17:47:47.597650Z","iopub.status.idle":"2024-04-11T17:47:47.708087Z","shell.execute_reply.started":"2024-04-11T17:47:47.597619Z","shell.execute_reply":"2024-04-11T17:47:47.706801Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"Introducing an output layer to the neural network, consisting of 7 neurons. Each neuron corresponds to a specific class in a multi-class classification task. To facilitate the classification decision, the softmax activation function is utilized. This function generates probability scores for each class, enabling the production of a probability distribution across the various classes.","metadata":{}},{"cell_type":"code","source":"cnn_model.add(Dense(7,activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:47:51.798847Z","iopub.execute_input":"2024-04-11T17:47:51.799296Z","iopub.status.idle":"2024-04-11T17:47:51.830041Z","shell.execute_reply.started":"2024-04-11T17:47:51.799265Z","shell.execute_reply":"2024-04-11T17:47:51.828595Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"Preparing the neural network model (cnn_model) for training. The optimizer used is, the loss function is sparse categorical crossentropy, and the evaluation metric is accuracy. This setting is required to train the model effectively on a dataset.","metadata":{}},{"cell_type":"code","source":"from keras.optimizers import Adam\ncnn_model.compile(optimizer=Adam(),loss='sparse_categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:05:03.541887Z","iopub.execute_input":"2024-04-11T18:05:03.542292Z","iopub.status.idle":"2024-04-11T18:05:03.554604Z","shell.execute_reply.started":"2024-04-11T18:05:03.542263Z","shell.execute_reply":"2024-04-11T18:05:03.553429Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Training the CNN model across 5 epochs with early stopping enabled. The training process is monitored based on the validation loss and training is terminated if the validation loss does not improve for 5 consecutive epochs. Early stopping is a technique for preventing overfitting that involves stopping the training process as soon as the model's performance on a validation dataset stops improving.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\ncnn_model.fit(x_train_reshaped,y_train_encoded,epochs=5,batch_size=32,validation_data=(x_val_reshaped,y_val_encoded),callbacks=EarlyStopping(monitor='val_loss',patience=5,verbose=1,mode='min'))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:06:26.327959Z","iopub.execute_input":"2024-04-11T18:06:26.328366Z","iopub.status.idle":"2024-04-11T18:17:23.391326Z","shell.execute_reply.started":"2024-04-11T18:06:26.328336Z","shell.execute_reply":"2024-04-11T18:17:23.390238Z"},"trusted":true},"execution_count":129,"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 159ms/step - accuracy: 0.3011 - loss: 1.7108 - val_accuracy: 0.3752 - val_loss: 1.5411\nEpoch 2/5\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 158ms/step - accuracy: 0.4098 - loss: 1.4988 - val_accuracy: 0.4231 - val_loss: 1.4677\nEpoch 3/5\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 158ms/step - accuracy: 0.4686 - loss: 1.3701 - val_accuracy: 0.4945 - val_loss: 1.3190\nEpoch 4/5\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 158ms/step - accuracy: 0.5230 - loss: 1.2671 - val_accuracy: 0.4928 - val_loss: 1.3098\nEpoch 5/5\n\u001b[1m816/816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 157ms/step - accuracy: 0.5447 - loss: 1.2009 - val_accuracy: 0.5166 - val_loss: 1.2699\n","output_type":"stream"},{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7bb19910a6e0>"},"metadata":{}}]},{"cell_type":"markdown","source":"Making predictions (cnn_pred) for the test dataset using the trained CNN model and then extract the predicted class labels (pred_lab) by taking the index of the highest probability prediction for each sample. This step evaluates the performance of the model and compares its predictions to the true labels for the test dataset.","metadata":{}},{"cell_type":"code","source":"cnn_pred=cnn_model.predict(x_test)\npred_lab=np.argmax(cnn_pred,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:18:05.948629Z","iopub.execute_input":"2024-04-11T18:18:05.949043Z","iopub.status.idle":"2024-04-11T18:18:15.411668Z","shell.execute_reply.started":"2024-04-11T18:18:05.949014Z","shell.execute_reply":"2024-04-11T18:18:15.410546Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Submsission:**\n\nThe final predictions of trained CNN model on the test data (x_test) are saved in a CSV file named 'cnn_submission.csv' where the submission file contains the '**id**' from 'te_data' and '**emotion**' from 'pred_lab'","metadata":{}},{"cell_type":"code","source":"submission_dnn=pd.DataFrame({'id':te_data['id'],'emotion':pred_lab})\nsubmission_dnn.to_csv('/kaggle/working/cnn_submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:18:18.904565Z","iopub.execute_input":"2024-04-11T18:18:18.905033Z","iopub.status.idle":"2024-04-11T18:18:18.924031Z","shell.execute_reply.started":"2024-04-11T18:18:18.905001Z","shell.execute_reply":"2024-04-11T18:18:18.922959Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"# **Results and Analysis:**\n\n**Overview of Model Performance:**\n\n\n| Models                       | Accuracy | Kaggle Score |\n|------------------------------|----------|--------------|\n| Support Vector Machine (SVM) | 0.42     | 0.42         |\n| Deep Neural Network (DNN)    | 0.3617   | 0.38         |\n| Convolutional Neural Network | 0.5166   | 0.52         |\n\n\n\n**Key findings and learnings:**\n\n* CNN surpasses SVM and DNN in emotion recognition because of its capability to understand spatial data.\n\n* The standardization of input features through data normalization enhances the training of the model.\n\n* The performance of CNN can be further enhanced by carefully selecting suitable architectures and hyperparameters. \n\n* Additionally, the implementation of early stopping helps in preventing overfitting during the training process.","metadata":{}},{"cell_type":"markdown","source":"# **Summary:**\n\nBased on the results and the performance of the models, it is shown that the CNN model is the most effective among the three. CNNs are specifically built to capture spatial hierarchies of characteristics, allowing them to learn complex patterns and representations more efficiently. Although SVMs are adaptable, they may struggle to capture detailed properties when compared to CNNs. Despite their depth, deep neural networks may lack the specialized architecture required, resulting in reduced accuracy in this context. The CNN model beats SVM and DNN resulting in higher accuracy. The CNN model's complexity grows in correlation with the quantity of layers, filters, neurons, and connections.","metadata":{}},{"cell_type":"markdown","source":"**References**\n\n1. S. Y. Chaganti, I. Nanda, K. R. Pandi, T. G. N. R. S. N. Prudhvith and N. Kumar, \"Image Classification using SVM and CNN,\" 2020 International Conference on Computer Science, Engineering and Applications (ICCSEA), Gunupur, India, 2020, pp. 1-5, doi: 10.1109/ICCSEA49143.2020.9132851.\n\n2. Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. [online] arXiv.org. Available at: https://arxiv.org/abs/1408.5882.\n\n3. ter Burg, K. and Kaya, H. (2022) ‘Comparing Approaches for Explaining DNN-Based Facial Expression Classifications’, Algorithms, 15(10), p. 367. Available at: https://doi.org/10.3390/a15100367.","metadata":{}}]}